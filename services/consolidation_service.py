"""
è®°å¿†å›ºåŒ–æœåŠ¡ (Consolidation Service)

åŸºäº He et al. (2024) çš„ç†è®ºï¼šè®°å¿†å›ºåŒ–åº”åœ¨ Session ç»“æŸåç¦»çº¿è¿è¡Œ
èåˆ CHI'24 Hou et al. çš„æƒ…æ„Ÿæ˜¾è‘—æ€§æå–

åŠŸèƒ½ï¼š
- L3: æå–ç”¨æˆ·ç”»åƒå¢é‡å¹¶æ›´æ–°ï¼ˆå«æƒ…æ„Ÿæ˜¾è‘—æ€§ï¼‰
- L4: æ‰¹é‡ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨ï¼ˆå«æƒ…æ„Ÿæ˜¾è‘—æ€§åˆ†æ•°ï¼‰
"""

import json
from typing import Dict, List, Optional
from datetime import datetime

from database import DBManager
from database.vector_store import VectorStore, get_vector_store
from config import Config


class ConsolidationService:
    """
    è®°å¿†å›ºåŒ–æœåŠ¡

    åœ¨æ¯æ¬¡ Sessionï¼ˆä»»åŠ¡ï¼‰ç»“æŸåè°ƒç”¨ï¼Œå°†çŸ­æœŸè®°å¿†è½¬åŒ–ä¸ºé•¿æœŸè®°å¿†
    """

    def __init__(self, db_manager: DBManager, llm_manager=None):
        """
        åˆå§‹åŒ–å›ºåŒ–æœåŠ¡

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            llm_manager: LLM ç®¡ç†å™¨ï¼ˆç”¨äºç”Ÿæˆæ‘˜è¦ï¼‰
        """
        self.db = db_manager
        self.llm = llm_manager
        self.vector_store = get_vector_store(db_manager)

    # ============ ä¸»å…¥å£ ============

    def consolidate_after_session(
        self,
        user_id: str,
        task_id: int,
        memory_group: str
    ) -> Dict:
        """
        Session ç»“æŸåçš„è®°å¿†å›ºåŒ–

        Args:
            user_id: ç”¨æˆ·ID
            task_id: åˆšç»“æŸçš„ä»»åŠ¡ID
            memory_group: è®°å¿†ç»„åˆ«

        Returns:
            å›ºåŒ–ç»“æœç»Ÿè®¡
        """
        stats = {
            'user_id': user_id,
            'task_id': task_id,
            'memory_group': memory_group,
            'timestamp': datetime.utcnow().isoformat(),
            'success': False,  # é»˜è®¤å¤±è´¥ï¼ŒæˆåŠŸåæ”¹ä¸º True
            'action': 'no_action',
            'error': None,
            'error_type': None
        }

        try:
            if memory_group == 'gist_memory':
                # L3: ç”¨æˆ·ç”»åƒå¢é‡æ›´æ–°
                print(f"[Consolidation] å¼€å§‹ L3 å›ºåŒ–: user={user_id}, task={task_id}")
                result = self._consolidate_gist(user_id, task_id)
                stats.update(result)

            elif memory_group == 'hybrid_memory':
                # L4: æ‰¹é‡å‘é‡ç”Ÿæˆ
                print(f"[Consolidation] å¼€å§‹ L4 å›ºåŒ–: user={user_id}, task={task_id}")
                result = self._consolidate_vectors(user_id, task_id)
                stats.update(result)

            else:
                # L1, L2 ä¸éœ€è¦å›ºåŒ–
                stats['action'] = 'no_consolidation_needed'

            stats['success'] = True
            print(f"[Consolidation] å›ºåŒ–æˆåŠŸ: {stats}")

        except Exception as e:
            # è¯¦ç»†é”™è¯¯è®°å½•
            stats['success'] = False
            stats['error'] = str(e)
            stats['error_type'] = type(e).__name__

            # é”™è¯¯åˆ†ç±»
            if 'API' in str(e) or 'timeout' in str(e).lower():
                stats['error_category'] = 'api_failure'
            elif 'JSON' in str(e) or 'json' in str(e).lower():
                stats['error_category'] = 'llm_output_parsing_error'
            elif 'database' in str(e).lower() or 'sql' in str(e).lower():
                stats['error_category'] = 'database_error'
            else:
                stats['error_category'] = 'unknown_error'

            # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            print(f"[Consolidation] âŒ å›ºåŒ–å¤±è´¥: {stats['error_category']}")
            print(f"[Consolidation] é”™è¯¯è¯¦æƒ…: {e}")
            import traceback
            traceback.print_exc()

            # è®°å½•åˆ°æ•°æ®åº“ï¼ˆå³ä½¿å›ºåŒ–å¤±è´¥ï¼Œä¹Ÿè¦è®°å½•æ—¥å¿—ï¼‰
            try:
                self.db.log_event(
                    user_id,
                    'consolidation_failed',
                    task_id=task_id,
                    event_data={
                        'error': str(e),
                        'error_type': stats['error_type'],
                        'error_category': stats['error_category'],
                        'memory_group': memory_group
                    }
                )
            except:
                # å¦‚æœè¿æ—¥å¿—éƒ½è®°ä¸äº†ï¼Œè‡³å°‘æ‰“å°å‡ºæ¥
                print(f"[Consolidation] âš ï¸ æ— æ³•è®°å½•å¤±è´¥æ—¥å¿—")

        return stats

    # ============ L3: ç”¨æˆ·ç”»åƒå¢é‡å›ºåŒ– ============

    def _consolidate_gist(self, user_id: str, task_id: int) -> Dict:
        """
        L3 å›ºåŒ–ï¼šæå–ç”¨æˆ·ç”»åƒå¢é‡å¹¶æ›´æ–°

        ç†è®ºä¾æ®ï¼šFuzzy Trace Theory
        - ä¿ç•™è¯­ä¹‰æœ¬è´¨ï¼ˆgistï¼‰
        - ä¸¢å¼ƒå­—é¢ç»†èŠ‚ï¼ˆverbatimï¼‰

        å®ç°ï¼š
        1. è¯»å–æœ¬æ¬¡ session çš„æ‰€æœ‰å¯¹è¯
        2. è°ƒç”¨ LLM æå–ç”¨æˆ·ç‰¹è´¨
        3. ä¸å·²æœ‰ç”»åƒåˆå¹¶
        4. æ›´æ–°æ•°æ®åº“
        """
        print(f"[Consolidation L3] å¼€å§‹å›ºåŒ–ç”¨æˆ·ç”»åƒ: user={user_id}, task={task_id}")

        # 1. è·å–æœ¬æ¬¡ session çš„æ¶ˆæ¯
        messages = self.db.get_task_messages(user_id, task_id)

        if not messages:
            return {'action': 'skip', 'reason': 'no_messages'}

        # 2. æ„å»ºå¯¹è¯æ–‡æœ¬
        conversation_text = self._format_messages_for_extraction(messages)

        # 3. è·å–å·²æœ‰ç”»åƒ
        existing_profile = self._get_user_profile(user_id)

        # 4. è°ƒç”¨ LLM æå–å¢é‡ç”»åƒ
        if self.llm and hasattr(self.llm, 'chat_completion'):
            profile_increment = self._extract_profile_increment(
                conversation_text,
                existing_profile,
                task_id  # ä¼ å…¥ä»»åŠ¡IDç”¨äºæº¯æºæ ‡æ³¨
            )
        else:
            # é™çº§æ–¹æ¡ˆï¼šè§„åˆ™æå–
            profile_increment = self._extract_profile_by_rules(conversation_text, task_id)

        # 5. åˆå¹¶ç”»åƒ
        updated_profile = self._merge_profiles(existing_profile, profile_increment)

        # 6. ä¿å­˜åˆ°æ•°æ®åº“
        self._save_user_profile(user_id, updated_profile, task_id)

        return {
            'action': 'gist_consolidated',
            'messages_processed': len(messages),
            'profile_fields_updated': len(profile_increment.keys()),
            'new_traits_count': sum(len(v) for v in profile_increment.values() if isinstance(v, list))
        }

    def _extract_profile_increment(
        self,
        conversation: str,
        existing_profile: Dict,
        task_id: int
    ) -> Dict:
        """
        ä½¿ç”¨ LLM æå–ç”¨æˆ·ç”»åƒå¢é‡ï¼ˆL3 å¢å¼ºç‰ˆï¼šå«æƒ…æ„Ÿæ˜¾è‘—æ€§ï¼‰

        æç¤ºè¯è®¾è®¡ï¼ˆèåˆCHI'24 Hou et al.çš„æƒ…æ„Ÿæ˜¾è‘—æ€§ï¼‰ï¼š
        - åªæå–**æ–°å¢**çš„ç‰¹è´¨ï¼ˆé¿å…é‡å¤ï¼‰
        - åˆ†ç±»å­˜å‚¨ï¼ˆåŸºæœ¬ä¿¡æ¯ã€åå¥½ã€é™åˆ¶ã€ç›®æ ‡ç­‰ï¼‰
        - ğŸ”´ æ–°å¢ï¼šæƒ…æ„Ÿæ˜¾è‘—æ€§æå–ï¼ˆæ·±å±‚æƒ…æ„Ÿéœ€æ±‚ã€æ ¸å¿ƒä»·å€¼è§‚ã€é«˜æƒ…æ„Ÿå¼ºåº¦äº‹ä»¶ï¼‰
        """
        # å°è¯•ä» config.py è¯»å–å¢å¼ºç‰ˆæç¤ºè¯
        enhanced_prompt_template = Config.GIST_CONFIG.get('profile_extraction_prompt', None)

        if enhanced_prompt_template:
            # ä½¿ç”¨å¢å¼ºç‰ˆæç¤ºè¯
            prompt = enhanced_prompt_template.format(
                existing_profile=json.dumps(existing_profile, ensure_ascii=False, indent=2),
                task_id=task_id,
                conversation=conversation
            )
        else:
            # é™çº§ï¼šä½¿ç”¨å†…ç½®çš„å¢å¼ºç‰ˆæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·ç”»åƒåˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯ï¼Œæå–ç”¨æˆ·çš„é•¿æœŸç‰¹è´¨ã€‚

**å·²çŸ¥ç”»åƒ**ï¼š
{json.dumps(existing_profile, ensure_ascii=False, indent=2)}

**æœ¬æ¬¡å¯¹è¯**ï¼ˆç¬¬ {task_id} æ¬¡ä»»åŠ¡ï¼‰ï¼š
{conversation}

**ä»»åŠ¡**ï¼š
1. æå–æœ¬æ¬¡å¯¹è¯ä¸­**æ–°å‡ºç°**çš„ç”¨æˆ·ç‰¹è´¨ï¼ˆä¸è¦é‡å¤å·²çŸ¥ç”»åƒï¼‰
2. **é‡è¦**ï¼šæ¯ä¸ªç‰¹è´¨åé¢å¿…é¡»æ ‡æ³¨æ¥æºä»»åŠ¡ï¼Œæ ¼å¼ä¸º "[Task N]"
3. æŒ‰ä»¥ä¸‹åˆ†ç±»æ•´ç†ï¼š
   - basic_info: åŸºæœ¬ä¿¡æ¯ï¼ˆå¹´é¾„ã€èŒä¸šã€èº«ä»½ç­‰ï¼‰
   - preferences: åå¥½å’Œå–œå¥½ï¼ˆé¥®é£Ÿã€çˆ±å¥½ã€å“å‘³ç­‰ï¼‰
   - constraints: é™åˆ¶å’Œçº¦æŸï¼ˆè¿‡æ•ã€æ—¶é—´é™åˆ¶ã€ç¦å¿Œç­‰ï¼‰
   - goals: ç›®æ ‡å’Œè®¡åˆ’ï¼ˆè¿‘æœŸç›®æ ‡ã€é•¿æœŸè§„åˆ’ç­‰ï¼‰
   - personality: æ€§æ ¼ç‰¹å¾ï¼ˆå†…å‘/å¤–å‘ã€å®Œç¾ä¸»ä¹‰ç­‰ï¼‰
   - social: ç¤¾äº¤å…³ç³»ï¼ˆå®¶äººã€æœ‹å‹ã€å® ç‰©ç­‰ï¼‰

4. **ğŸ”´ æƒ…æ„Ÿæ˜¾è‘—æ€§æå–**ï¼ˆé‡è¦ï¼è¿™æœ‰åŠ©äºAIå±•ç°æ›´æ·±å±‚çš„"ç†è§£æ„Ÿ"ï¼‰ï¼š
   - emotional_needs: ç”¨æˆ·è¡¨è¾¾çš„**æ·±å±‚æƒ…æ„Ÿéœ€æ±‚**ï¼ˆå¦‚è¢«ç†è§£ã€è¢«è®¤å¯ã€å®‰å…¨æ„Ÿã€å½’å±æ„Ÿç­‰ï¼‰
   - core_values: ç”¨æˆ·é€éœ²çš„**æ ¸å¿ƒä»·å€¼è§‚**ï¼ˆå¦‚å®¶åº­ä¼˜å…ˆã€äº‹ä¸šå¯¼å‘ã€å¥åº·æ„è¯†ã€è‡ªç”±è¿½æ±‚ç­‰ï¼‰
   - significant_events: **é«˜æƒ…æ„Ÿå¼ºåº¦äº‹ä»¶**ï¼ˆå¦‚é‡å¤§å†³å®šã€äººç”Ÿè½¬æŠ˜ã€æƒ…ç»ªæ³¢åŠ¨æ—¶åˆ»ï¼Œæ ‡æ³¨æƒ…æ„Ÿç±»å‹ï¼šå–œ/æ€’/å“€/æƒ§/æœŸå¾…/å¤±æœ›ç­‰ï¼‰

**è¾“å‡ºæ ¼å¼ç¤ºä¾‹**ï¼ˆçº¯ JSONï¼Œä¸è¦è§£é‡Šï¼‰ï¼š
{{
  "basic_info": {{"occupation": "åšå£«ç”Ÿ [Task 1]"}},
  "preferences": ["å–œæ¬¢çˆ¬å±± [Task 1]", "ç´ é£Ÿä¸»ä¹‰è€… [Task 1]"],
  "constraints": ["å¯¹æµ·é²œè¿‡æ• [Task 1]"],
  "goals": ["å‡†å¤‡è€ƒåš [Task 1]"],
  "personality": ["å†…å‘ [Task 1]"],
  "social": ["å…»äº†ä¸€åªçŒ« [Task 1]"],
  "emotional_needs": ["å¸Œæœ›è¢«ç†è§£å’Œè®¤å¯ [Task 1]", "éœ€è¦ç‹¬å¤„ç©ºé—´ [Task 1]"],
  "core_values": ["å­¦æœ¯è¿½æ±‚ [Task 1]", "å¥åº·ç”Ÿæ´» [Task 1]"],
  "significant_events": ["å¯¹æœªæ¥èŒä¸šæ–¹å‘æ„Ÿåˆ°è¿·èŒ«ï¼ˆç„¦è™‘ï¼‰ [Task 1]"]
}}

å¦‚æœæœ¬æ¬¡å¯¹è¯æ²¡æœ‰æ–°ç‰¹è´¨ï¼Œè¿”å›ç©º JSON {{}}.
"""

        try:
            # è°ƒç”¨ LLMï¼ˆå¢åŠ è¶…æ—¶æ§åˆ¶ï¼‰
            response = self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # ä½æ¸©åº¦ä¿è¯è¾“å‡ºç¨³å®š
                max_tokens=800
            )

            # æ¸…ç†å“åº”ï¼ˆç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°ï¼‰
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.startswith('```'):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            cleaned_response = cleaned_response.strip()

            # è§£æ JSON
            profile_increment = json.loads(cleaned_response)

            # éªŒè¯æ ¼å¼
            if not isinstance(profile_increment, dict):
                print(f"[Consolidation] âš ï¸ LLM è¿”å›éå­—å…¸æ ¼å¼ï¼Œä½¿ç”¨ç©ºç”»åƒ")
                return {}

            return profile_increment

        except json.JSONDecodeError as e:
            print(f"[Consolidation] âŒ LLM è¿”å› JSON æ ¼å¼é”™è¯¯: {e}")
            print(f"[Consolidation] åŸå§‹è¾“å‡ºï¼ˆå‰ 500 å­—ç¬¦ï¼‰:")
            print(response[:500] if 'response' in locals() else "æ— å“åº”")
            return {}

        except TimeoutError:
            print(f"[Consolidation] âŒ LLM è°ƒç”¨è¶…æ—¶")
            return {}

        except Exception as e:
            print(f"[Consolidation] âŒ LLM è°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}")
            return {}

    def _extract_profile_by_rules(self, conversation: str, task_id: int) -> Dict:
        """
        é™çº§æ–¹æ¡ˆï¼šåŸºäºè§„åˆ™çš„ç”»åƒæå–ï¼ˆå½“ LLM ä¸å¯ç”¨æ—¶ï¼‰

        ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…
        """
        profile = {
            "preferences": [],
            "constraints": [],
            "goals": []
        }

        # åå¥½å…³é”®è¯
        preference_patterns = [
            ("å–œæ¬¢", "preferences"),
            ("çˆ±å¥½", "preferences"),
            ("æœ€çˆ±", "preferences"),
            ("ä¸å–œæ¬¢", "constraints"),
            ("è®¨åŒ", "constraints"),
            ("è¿‡æ•", "constraints"),
            ("æƒ³è¦", "goals"),
            ("æ‰“ç®—", "goals"),
            ("è®¡åˆ’", "goals")
        ]

        lines = conversation.split('\n')
        for line in lines:
            if line.startswith('ç”¨æˆ·ï¼š'):
                content = line[3:].strip()

                for pattern, category in preference_patterns:
                    if pattern in content:
                        # ç®€å•æå–ï¼ˆå®é™…åº”è¯¥ç”¨ NERï¼‰
                        trait = content[:50]  # æˆªå–å‰ 50 å­—
                        # æ·»åŠ æº¯æºæ ‡æ³¨
                        trait_with_source = f"{trait} [Task {task_id}]"
                        if trait and trait_with_source not in profile[category]:
                            profile[category].append(trait_with_source)

        return profile

    def _merge_profiles(self, existing: Dict, increment: Dict) -> Dict:
        """
        åˆå¹¶å·²æœ‰ç”»åƒå’Œå¢é‡ç”»åƒ

        ç­–ç•¥ï¼š
        - å­—å…¸ç±»å‹ï¼šæ›´æ–°é”®å€¼
        - åˆ—è¡¨ç±»å‹ï¼šå»é‡è¿½åŠ 
        """
        merged = existing.copy()

        for key, value in increment.items():
            if key not in merged:
                merged[key] = value
            else:
                if isinstance(value, dict):
                    # åˆå¹¶å­—å…¸
                    merged[key].update(value)
                elif isinstance(value, list):
                    # å»é‡è¿½åŠ åˆ—è¡¨
                    existing_set = set(merged[key])
                    for item in value:
                        if item not in existing_set:
                            merged[key].append(item)

        return merged

    def _get_user_profile(self, user_id: str) -> Dict:
        """è·å–ç”¨æˆ·ç°æœ‰ç”»åƒ"""
        return self.db.get_user_profile(user_id)

    def _save_user_profile(self, user_id: str, profile: Dict, task_id: int):
        """ä¿å­˜ç”¨æˆ·ç”»åƒ"""
        print(f"[Consolidation] ç”»åƒå·²æ›´æ–°:")
        print(json.dumps(profile, ensure_ascii=False, indent=2))

        # ä¿å­˜åˆ°æ•°æ®åº“
        self.db.save_user_profile(user_id, profile, task_id)

    def _format_messages_for_extraction(self, messages) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯ç”¨äºç”»åƒæå–"""
        lines = []
        for msg in messages:
            role = "ç”¨æˆ·" if msg.is_user else "AI"
            lines.append(f"{role}ï¼š{msg.content}")
        return "\n".join(lines)

    # ============ L4: å‘é‡æ‰¹é‡å›ºåŒ– ============

    def _consolidate_vectors(self, user_id: str, task_id: int) -> Dict:
        """
        L4 å›ºåŒ–ï¼šæ‰¹é‡ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨

        ç†è®ºä¾æ®ï¼šTulving é™ˆè¿°æ€§è®°å¿†
        - å‘é‡åŒ–åçš„è®°å¿†æ”¯æŒè¯­ä¹‰æ£€ç´¢

        å®ç°ï¼š
        1. æ‰¾åˆ°æœ¬æ¬¡ session ä¸­æœªå‘é‡åŒ–çš„æ¶ˆæ¯
        2. æ‰¹é‡è°ƒç”¨ DashScope Embedding API
        3. æ›´æ–° chat_messages.embedding å­—æ®µ
        """
        print(f"[Consolidation L4] å¼€å§‹æ‰¹é‡å‘é‡åŒ–: user={user_id}, task={task_id}")

        # 1. è·å–æœªå‘é‡åŒ–çš„æ¶ˆæ¯
        messages = self.db.get_task_messages(user_id, task_id)

        if not messages:
            return {'action': 'skip', 'reason': 'no_messages'}

        # è¿‡æ»¤å‡ºæœªå‘é‡åŒ–çš„æ¶ˆæ¯
        unvectorized = [msg for msg in messages if not msg.embedding]

        if not unvectorized:
            return {
                'action': 'vectors_already_exist',
                'total_messages': len(messages)
            }

        # 2. æ‰¹é‡ç”Ÿæˆå‘é‡
        texts = [msg.content for msg in unvectorized]
        embeddings = self.vector_store.generate_embeddings_batch(texts)

        # 3. æ›´æ–°æ•°æ®åº“ï¼ˆå«æƒ…æ„Ÿæ˜¾è‘—æ€§ï¼‰
        success_count = 0
        fail_count = 0

        for msg, embedding in zip(unvectorized, embeddings):
            if embedding:
                # è®¡ç®—é‡è¦æ€§åˆ†æ•°ï¼ˆç®€å•è§„åˆ™ï¼‰
                importance = self._calculate_importance(msg.content, msg.is_user)

                # è®¡ç®—æƒ…æ„Ÿæ˜¾è‘—æ€§ï¼ˆCHI'24 å¢å¼ºï¼‰
                emotional_salience = self._calculate_emotional_salience(msg.content, msg.is_user)

                # æ›´æ–°æ•°æ®åº“
                if self._update_message_with_embedding_and_salience(
                    msg.message_id,
                    embedding,
                    importance,
                    emotional_salience
                ):
                    success_count += 1
                else:
                    fail_count += 1
            else:
                fail_count += 1

        return {
            'action': 'vectors_consolidated',
            'total_messages': len(messages),
            'unvectorized_before': len(unvectorized),
            'success': success_count,
            'failed': fail_count
        }

    def _update_message_with_embedding_and_salience(
        self,
        message_id: str,
        embedding: list,
        importance_score: float,
        emotional_salience: float
    ) -> bool:
        """
        æ›´æ–°æ¶ˆæ¯çš„å‘é‡ã€é‡è¦æ€§å’Œæƒ…æ„Ÿæ˜¾è‘—æ€§

        Args:
            message_id: æ¶ˆæ¯ID
            embedding: å‘é‡
            importance_score: é‡è¦æ€§åˆ†æ•°
            emotional_salience: æƒ…æ„Ÿæ˜¾è‘—æ€§åˆ†æ•°

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        from database import ChatMessage

        try:
            msg = self.db.session.query(ChatMessage).filter(
                ChatMessage.message_id == message_id
            ).first()

            if msg:
                msg.embedding = json.dumps(embedding)
                msg.importance_score = importance_score
                # æ›´æ–°æƒ…æ„Ÿæ˜¾è‘—æ€§å­—æ®µ
                if hasattr(msg, 'emotional_salience'):
                    msg.emotional_salience = emotional_salience
                self.db.session.commit()
                return True

            return False

        except Exception as e:
            print(f"[Consolidation] æ›´æ–°æ¶ˆæ¯å¤±è´¥: {e}")
            self.db.session.rollback()
            return False

    def _calculate_importance(self, content: str, is_user: bool) -> float:
        """
        è®¡ç®—æ¶ˆæ¯çš„é‡è¦æ€§åˆ†æ•°

        ç®€å•è§„åˆ™ï¼ˆå¯ä»¥æ”¹ä¸º LLM åˆ¤æ–­ï¼‰ï¼š
        - ç”¨æˆ·çš„è‡ªæˆ‘æŠ«éœ²ï¼šé«˜é‡è¦æ€§
        - åŒ…å«å…³é”®è¯ï¼ˆå§“åã€æƒ…æ„Ÿã€å†³ç­–ï¼‰ï¼šé«˜é‡è¦æ€§
        - AI çš„å›å¤ï¼šä¸­ç­‰é‡è¦æ€§
        """
        importance = 0.5  # åŸºç¡€åˆ†

        if is_user:
            importance += 0.2  # ç”¨æˆ·æ¶ˆæ¯æ›´é‡è¦

            # å…³é”®è¯åŒ¹é…
            high_importance_keywords = [
                'æˆ‘æ˜¯', 'æˆ‘å«', 'æˆ‘çš„', 'æˆ‘è§‰å¾—', 'æˆ‘è®¤ä¸º', 'æˆ‘å†³å®š',
                'å–œæ¬¢', 'è®¨åŒ', 'å¸Œæœ›', 'æ‹…å¿ƒ', 'å®³æ€•', 'å¼€å¿ƒ', 'éš¾è¿‡'
            ]

            if any(kw in content for kw in high_importance_keywords):
                importance += 0.2

            # é•¿æ¶ˆæ¯æ›´å¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯
            if len(content) > 50:
                importance += 0.1

        # é™åˆ¶åœ¨ 0-1 èŒƒå›´
        return min(1.0, importance)

    def _calculate_emotional_salience(self, content: str, is_user: bool) -> float:
        """
        è®¡ç®—æ¶ˆæ¯çš„æƒ…æ„Ÿæ˜¾è‘—æ€§åˆ†æ•°ï¼ˆCHI'24 å¢å¼ºï¼‰

        æƒ…æ„Ÿæ˜¾è‘—æ€§åæ˜ æ¶ˆæ¯çš„æƒ…æ„Ÿå¼ºåº¦å’Œæ·±åº¦ï¼Œç”¨äºï¼š
        1. L3 ç”»åƒæå–æ—¶è¯†åˆ«é«˜æƒ…æ„Ÿå¼ºåº¦äº‹ä»¶
        2. L4 å‘é‡æ£€ç´¢æ—¶æå‡æƒ…æ„Ÿç›¸å…³è®°å¿†çš„æƒé‡

        è§„åˆ™ï¼š
        - é«˜æƒ…æ„Ÿå¼ºåº¦è¯æ±‡ï¼š+0.3
        - è‡ªæˆ‘æŠ«éœ²è¯æ±‡ï¼š+0.2
        - ä»·å€¼è§‚ç›¸å…³è¯æ±‡ï¼š+0.1
        """
        salience = 0.0

        if not is_user:
            return 0.0  # AIæ¶ˆæ¯çš„æƒ…æ„Ÿæ˜¾è‘—æ€§ä¸º0

        # é«˜æƒ…æ„Ÿå¼ºåº¦è¯æ±‡
        high_emotion_keywords = [
            # å–œ
            'å¤ªå¼€å¿ƒäº†', 'å¤ªé«˜å…´äº†', 'å…´å¥‹', 'æ¿€åŠ¨', 'æ„ŸåŠ¨', 'å¹¸ç¦', 'æ»¡è¶³',
            # æ€’
            'ç”Ÿæ°”', 'æ„¤æ€’', 'æ°”æ­»', 'çƒ¦æ­»', 'è®¨åŒ', 'å—ä¸äº†',
            # å“€
            'éš¾è¿‡', 'ä¼¤å¿ƒ', 'å¤±è½', 'æ²®ä¸§', 'ç»æœ›', 'å¿ƒç—›', 'æƒ³å“­', 'å´©æºƒ',
            # æƒ§
            'å®³æ€•', 'ææƒ§', 'æ‹…å¿ƒ', 'ç„¦è™‘', 'ç´§å¼ ', 'ä¸å®‰', 'å‹åŠ›',
            # æœŸå¾…
            'æœŸå¾…', 'ç›¼æœ›', 'å¸Œæœ›', 'æƒ³è¦', 'æ¢¦æƒ³',
            # å¤±æœ›
            'å¤±æœ›', 'é—æ†¾', 'å¯æƒœ', 'åæ‚”'
        ]

        # è‡ªæˆ‘æŠ«éœ²è¯æ±‡
        self_disclosure_keywords = [
            'å…¶å®æˆ‘', 'è¯´å®è¯', 'è€å®è¯´', 'è·Ÿä½ è¯´', 'å‘Šè¯‰ä½ ',
            'ä»æ¥æ²¡', 'ç¬¬ä¸€æ¬¡', 'ä¸€ç›´ä»¥æ¥', 'å†…å¿ƒ', 'çœŸæ­£çš„æˆ‘'
        ]

        # ä»·å€¼è§‚ç›¸å…³è¯æ±‡
        value_keywords = [
            'æœ€é‡è¦', 'æœ€åœ¨ä¹', 'ä¸€å®šè¦', 'ç»å¯¹ä¸', 'åŸåˆ™', 'åº•çº¿',
            'æ„ä¹‰', 'ä»·å€¼', 'äººç”Ÿ', 'ç†æƒ³', 'ä¿¡å¿µ'
        ]

        # è®¡ç®—åˆ†æ•°
        if any(kw in content for kw in high_emotion_keywords):
            salience += 0.3

        if any(kw in content for kw in self_disclosure_keywords):
            salience += 0.2

        if any(kw in content for kw in value_keywords):
            salience += 0.1

        # æ„Ÿå¹å·å’Œé—®å·ä¹Ÿå¯èƒ½è¡¨ç¤ºæƒ…æ„Ÿå¼ºåº¦
        exclamation_count = content.count('ï¼') + content.count('!')
        question_count = content.count('ï¼Ÿ') + content.count('?')
        if exclamation_count >= 2:
            salience += 0.1
        if question_count >= 2:
            salience += 0.05

        # é™åˆ¶åœ¨ 0-1 èŒƒå›´
        return min(1.0, salience)

    # ============ å·¥å…·æ–¹æ³• ============

    def get_consolidation_stats(self, user_id: str) -> Dict:
        """è·å–å›ºåŒ–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'user_id': user_id
        }

        # L3 ç”»åƒç»Ÿè®¡
        profile = self._get_user_profile(user_id)
        stats['profile_traits_count'] = sum(
            len(v) if isinstance(v, list) else 1
            for v in profile.values()
        )

        # L4 å‘é‡ç»Ÿè®¡
        vector_stats = self.vector_store.get_stats(user_id)
        stats.update(vector_stats)

        return stats
